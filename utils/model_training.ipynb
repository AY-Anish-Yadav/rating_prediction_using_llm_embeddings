{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (huge dataset)\n",
    "\n",
    "### Way1: \n",
    "1. Pandas with chunk size \n",
    "2. del unnecesary information, gc.collect() \n",
    "\n",
    "\n",
    "--->RuntimeError: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4374528000 bytes.\n",
    "\n",
    "### Way2\n",
    "\n",
    "1. Load and process the data in chunks.\n",
    "2. Embed the reviews using the UAE large v1 model.\n",
    "3. Save each chunk's embeddings and ratings to a temporary file.\n",
    "4. Combine all temporary files into a single NumPy array at the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Generation\n",
    "Looked into --->  MTEB Leaderboard (massive text embedding benchmark)\n",
    "### UAE - large-V1 (universal angie embeddings)\n",
    "1. Dimensions : 1024\n",
    "2. Time : took 1 hr for 3k records.\n",
    "3. Model Size : 1.25 GB (fp32)\n",
    "4. Context length (tokens) : 512\n",
    "5. price : open source\n",
    "\n",
    "### Voyage-large-2-instruct\n",
    "1. Dimensions : 1024\n",
    "2. Time : \n",
    "3. Model Size : \n",
    "4. Context length (tokens) : 16k\n",
    "5. price : $ 0.12 / 1 million tokens\n",
    "\n",
    "### text-embedding-3-small\n",
    "1. Dimensions : 1536\n",
    "2. Time : \n",
    "3. Model Size : \n",
    "4. Context length (tokens) : 512\n",
    "5. price : $ 0.02 / 1 million tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Latency - Performance Trade - off :\n",
    "Delay between a user's action and the response from a system is known as an latency.\n",
    "2. Capturing Complexity of data - operational efficiency trade off:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train your own embedding model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .npy file\n",
    "\n",
    "import numpy as np\n",
    "file=r\"D:\\projects_llm\\review_prediction_using_llm_embeddings\\temp_chunks\\ratings_embeddings.npy\"\n",
    "data=np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 1025)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(data))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1025,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.        , -0.03100718, -0.00820842, ...,  0.01742723,\n",
       "        0.03722996,  0.02347247])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = data[:,0]   # shape (10,000,)\n",
    "embeddings = data[:,1:] # shape (10,000,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(embeddings,ratings,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 'Ridge Regression'\n",
    "# alpha: Regularization strength. Larger values specify stronger regularization. \n",
    "# The range of values here allows the model to try different levels of regularization to find the optimal one.\n",
    "\n",
    "## 'Lasso Regression'\n",
    "#alpha: Similar to Ridge Regression, alpha controls the regularization strength in Lasso Regression. \n",
    "# The values provided let the model explore different regularization strengths.\n",
    "\n",
    "## 'Random Forest Regressor'\n",
    "# n_estimators: The number of trees in the forest. Testing with different values (50, 100, 200) helps find the optimal number of trees.\n",
    "#max_depth: The maximum depth of each tree. Limiting the depth can prevent overfitting.\n",
    "#  None means nodes are expanded until all leaves are pure or contain fewer than the minimum samples.\n",
    "\n",
    "## 'SVM Regressor'\n",
    "# C: Regularization parameter. The strength of the regularization is inversely proportional to C. Smaller values specify stronger regularization.\n",
    "# epsilon: Specifies the epsilon-tube within which no penalty is associated in the training loss function.\n",
    "# kernel: Specifies the kernel type to be used in the algorithm. 'linear' uses a linear kernel, while 'rbf' uses a radial basis function kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of regressors and their parameter grids for hyperparameter tuning\n",
    "models = {\n",
    "    'Linear Regression': (LinearRegression(), {}),\n",
    "    'Ridge Regression': (Ridge(), {'alpha': [0.1, 1, 10]}),\n",
    "    'Lasso Regression': (Lasso(), {'alpha': [0.1, 1, 10]}),\n",
    "    'Random Forest Regressor': (RandomForestRegressor(), {'n_estimators': [10, 20], 'max_depth': [None, 10, 20]}),\n",
    "    'SVM Regressor': (SVR(), {'C': [0.1, 1, 10], 'epsilon': [0.01, 0.1, 0.2]}) # kernel : linear, RBF\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n",
    "1. GridSearchCV:\n",
    "\n",
    "GridSearchCV is a function from scikit-learn used to perform an exhaustive search over specified parameter values for an estimator. It helps in finding the best combination of hyperparameters for a model.\n",
    "Parameters:\n",
    "\n",
    "a. model: The machine learning model/estimator to be optimized (e.g., LinearRegression(), Ridge(), SVR(), etc.).\n",
    "\n",
    "b. params: A dictionary where keys are the hyperparameter names and values are lists of values to try. For example, {'alpha': [0.1, 1, 10]} for Ridge Regression.\n",
    "\n",
    "c. cv=5: This sets up cross-validation with 5 folds. The data will be split into 5 parts, and the model will be trained and validated 5 times, each time using a different part of the data as the validation set and the remaining parts as the training set.\n",
    "\n",
    "d. scoring='neg_mean_squared_error': The scoring method to evaluate the predictions. neg_mean_squared_error is used because GridSearchCV expects a score to maximize. By using the negative MSE, it effectively minimizes the MSE.\n",
    "\n",
    "e. n_jobs=-1: This parameter allows the search to use all available CPU cores to parallelize the computation, speeding up the process.\n",
    "Fit the Grid Search:\n",
    "\n",
    "2. grid_search.fit(X_train, y_train): This line performs the grid search on the training data. It tries all combinations of parameters specified in params using cross-validation and evaluates them using the scoring method. The best combination of parameters is selected based on the cross-validation performance.\n",
    "Best Estimator:\n",
    "\n",
    "3. best_model = grid_search.best_estimator_: After fitting the grid search, grid_search.best_estimator_ contains the model with the best combination of hyperparameters found during the search.\n",
    "Store the Best Model:\n",
    "\n",
    "4. best_models[name] = best_model: This line stores the best model found for the current regressor in the best_models dictionary, using the name of the regressor as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Best Parameters: {}\n",
      "Linear Regression - Mean Squared Error: 0.609811512037517\n",
      "Linear Regression - R2 Score: 0.6989985696544256\n",
      "Ridge Regression - Best Parameters: {'alpha': 1}\n",
      "Ridge Regression - Mean Squared Error: 0.5744906312561676\n",
      "Ridge Regression - R2 Score: 0.7164328676733807\n",
      "Lasso Regression - Best Parameters: {'alpha': 0.1}\n",
      "Lasso Regression - Mean Squared Error: 2.0261129777777778\n",
      "Lasso Regression - R2 Score: -8.427573817559875e-05\n",
      "Random Forest Regressor - Best Parameters: {'max_depth': 10, 'n_estimators': 20}\n",
      "Random Forest Regressor - Mean Squared Error: 0.6689710396708786\n",
      "Random Forest Regressor - R2 Score: 0.6697975754378473\n",
      "SVM Regressor - Best Parameters: {'C': 1, 'epsilon': 0.2}\n",
      "SVM Regressor - Mean Squared Error: 0.5456886564468697\n",
      "SVM Regressor - R2 Score: 0.7306494500816224\n"
     ]
    }
   ],
   "source": [
    "best_models = {}\n",
    "from joblib import dump\n",
    "# Train, tune, and evaluate each model\n",
    "for name, (model, params) in models.items():\n",
    "    grid_search = GridSearchCV(model, params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models[name] = best_model\n",
    "    \n",
    "    y_pred = best_model.predict(x_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f'{name} - Best Parameters: {grid_search.best_params_}')\n",
    "    print(f'{name} - Mean Squared Error: {mse}')\n",
    "    print(f'{name} - R2 Score: {r2}')\n",
    "\n",
    "# Save the best model for each regressor\n",
    "for name, model in best_models.items():\n",
    "    dump(model, f'{name.replace(\" \", \"_\").lower()}_best_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SAYALI\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 2.0618 - root_mean_squared_error: 1.3510 - val_loss: 0.5532 - val_root_mean_squared_error: 0.7438\n",
      "Epoch 2/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.5923 - root_mean_squared_error: 0.7693 - val_loss: 0.5329 - val_root_mean_squared_error: 0.7300\n",
      "Epoch 3/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.5247 - root_mean_squared_error: 0.7242 - val_loss: 0.5246 - val_root_mean_squared_error: 0.7243\n",
      "Epoch 4/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.4993 - root_mean_squared_error: 0.7059 - val_loss: 0.5245 - val_root_mean_squared_error: 0.7242\n",
      "Epoch 5/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.4488 - root_mean_squared_error: 0.6698 - val_loss: 0.5520 - val_root_mean_squared_error: 0.7430\n",
      "Epoch 6/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.4377 - root_mean_squared_error: 0.6610 - val_loss: 0.5539 - val_root_mean_squared_error: 0.7443\n",
      "Epoch 7/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.4318 - root_mean_squared_error: 0.6570 - val_loss: 0.5769 - val_root_mean_squared_error: 0.7595\n",
      "Epoch 8/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.3669 - root_mean_squared_error: 0.6056 - val_loss: 0.5836 - val_root_mean_squared_error: 0.7639\n",
      "Epoch 9/30\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.3082 - root_mean_squared_error: 0.5549 - val_loss: 0.6201 - val_root_mean_squared_error: 0.7875\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.6643698558188752\n",
      "R2 Score: 0.6720687082278934\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# Define the ANN model\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(x_train.shape[1],)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=9, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(x_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R2 Score: {r2}')\n",
    "# Save the model\n",
    "model.save('ann_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization\n",
    "1. Zero Initialization\n",
    "    Description: All weights are initialized to zero.\n",
    "    Usage: Generally not recommended because it can lead to symmetry issues where neurons learn the same features and gradients become identical.\n",
    "\n",
    "2. Random Initialization\n",
    "    Description: Weights are initialized to small random values. This can be done using uniform or normal distributions.\n",
    "    Usage: Basic method for initializing weights but might not always lead to effective training.\n",
    "\n",
    "3. Glorot (Xavier) Initialization\n",
    "    Description: Designed to maintain the variance of activations and gradients throughout the network. It scales the weights by the number of input and output units.\n",
    "    Usage: Suitable for activation functions like sigmoid and tanh.\n",
    "\n",
    "4. He Initialization\n",
    "    Description: Similar to Glorot but designed specifically for layers with ReLU activation functions. It scales weights by the number of input units.\n",
    "    Usage: Preferred for ReLU and its variants (e.g., Leaky ReLU).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
